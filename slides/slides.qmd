---
title: "Pseudoproxy modelling of uncertainties in palaeoecological data."
subtitle: "[Link to slides](https://quinnasena.github.io/palaeo_in_R_2022_talk_QA/slides/slides.html#/title-slide)"
author: "Quinn Asena, George Perry, Janet Wilmshurst"
institute: "University of Auckland, UW-Madison"
date: today
bibliography: refs.bib
from: markdown+emoji
format:
  revealjs:
    theme: default
#    backgroundcolor: "#44444c"
#    fontcolor: "white"
    preview-links: true
    fig-align: center
    highlight-style: github
    slide-number: c/t
    width: 1600
    height: 900
title-slide-attributes:
  data-background-color: "#1a1e43"
---

## The problem

Proxy data are the product of multiple sources of uncertainty

- Environmental processes

  - bioturbation, taphonomy, variable sedimentation rates...

- Field and laboratory methods

  - core collection methods, sub-sampling strategy, pollen counting...

- Data processing methods

  - age-depth modelling, interpolation...


## The question: is the past recoverable from the data?

::::{.columns}
:::{.column}
### Why it matters

- Palaeoecology moving from descriptive to quantitative

- Palaeoecology to inform the future requires robust statistical approaches

- Advances in lab methods, data availability, and statistics is making more
:::

:::{.column}
### What we can do about it

- One method to assess uncertainties is in simulation

:::
::::

## Approach and take-home

- Simulate core samples containing proxies that mimic the statistical properties of empirical data

- Simulate uncertainties: process and observer error that affect the data

- Assess how *statistical inferences* are affected by those uncertainties


## Key concepts

- Virtual ecology [@zurell2010]

- Proxy system modelling [@evans2013]

- Pseudoproxy experiments [@mann2002]

- Other key refs: 
    - @blaauw2010a

    - @williams2011

## Virtual ecology

Virtual ecology is a framework for assessing sampling and analytical methods in simulation consisting of:

1. an ecological model that generates synthetic data

    1a. a degradation model

2. a simulated observational process (a sampling model) that samples the synthetic data

3. an analytical process or statistical model applied to both sets of data

4. an assessment of the results

## Virtual ecology and empirical ecology

:::: {.columns}
::: {.column width="50%"}
#### Perfect knowledege, imperfect world

- known drivers and responses

- known environmental and observational processes

- Advantage of benchmark/control data

- Advantage of replication

- Able to systematically introduce uncertainty
:::

::: {.column width="50%"}
#### Perfect world, imperfect knowledge

- Sampled data with no benchmark/control

- Advantage of being reality

:::
::::

## Proxy system modelling

Describes the process by which environmental change is recorded as an observable signal in an archive:

1. environmental drivers (e.g., climatic variability)

2. a sensor (a physical, biological or chemical component of the system that responds to the environmental drivers)

3. an archive (the medium in which the response of the sensor is recorded such as a lake sediment)

4. observations drawn from the archive

![Proxy Ststem Modelling framework, from @evans2013](./images/psm.png)


## Pseudoproxy experiments

Borrowing the term "*pseudoproxies*" from climatology:

- pseudoproxies are simulated data or modified observational data

- mimic the statistical properties of empirical data

- pseudoproxy experiments are similar to virtual ecology


## Simulating a pseudoproxy archive {visibility="hidden"}

Simulate:

1. environmental driver patterns over time (driver model)

2. species that respond to the drivers (sensor model)

3. accumulation rate and time-span and degradation (archive model)

4. observational process (sampling model)

5. analyse the pseudoproxies (assessment)

# Building the model

## Simulating pseudoproxies {.scrollable}

We set out to:

- Represent multiple interacting drivers

- Include underlying ecological dynamics that can undergo community turnover

- Generate a multi-species pseudoproxy

- Recreate core formation processes of accumulation rates and time-span

- Virtually recreate the observational processes


## Simulating pseudoproxies {.scrollable}

:::{.panel-tabset}
# Model visualistion
![](./images/walk_lin_gradual_grid_plot.png){width=60% fig-align="center"}

# 'Error-free' output
![Partial output of 20 randomly selected species](./images/taxa_plot_N.png){width="90%"}
:::

## Ruining pseudoproxies
:::: {.columns}
::: {.column width="30%"}
A. Example species from single replicate

B. Mixing

C. Mixing + sub-sampling

D. Mixing + sub-sampling + proxy counting
:::

::: {.column width="70%"}
![](./images/degradation_process_disturbed_for_export.png){width="80%" fig-align="right"}
:::
::::

## Ruined pseudoproxies {.scrollable}

![](./images/taxa_plot_mix10_sam10_cnt400.png){height="120%"}

# Applying the model

## Analyses

Ok, now we have generated the data, let's analyse it. Two analyses:

- Fisher Information

- Principal curves

Demonstrating two scenarios with different driving environments.

## Scenario 1 {.scrollable}

:::{.panel-tabset}
# Pseudoproxies
![](./images/archive_half_half_walk_lin_forces.png){width="200%" fig-align="center"}

# Fisher Information
![](./images/fi_plot.png){width="80%" fig-align="center"}

# Principal curves
![](./images/prc_pc_plot.png){width="80%" fig-align="center"}
:::

## Scenario 2 {.scrollable}

:::{.panel-tabset}
# Pseudoproxies
![](./images/archive_half_half_abrupt_forces.png){width="200%" fig-align="center"}

# Fisher Information
![](./images/fi_plot_abrupt.png){width="80%" fig-align="center"}

# Principal curves
![](./images/prc_pc_plot_abrupt.png){width="80%" fig-align="center"}
:::


## Assessing uncertainties across replicates

 Each replicate results in 1210 datasets from the 'error-free' to the most uncertain, per scenario :scream:.

::::{.columns}
:::{.column width="40%"}
Across *replicates* for each of the 1210 datasets:

1. extract features from the FI and PrC

    - feature analysis reduces the FI and PrC to one dimension

2. calculate the distance between each dataset from the 'error-free' to the most uncertain

3. make cool visulisations!
:::

:::{.column width="60%"}
![](./images/miracle_occurs.png){width="80%"}
:::
::::

## Assessing uncertainties across replicates

**Quantify the difference between the 'error-free' archive and each level of uncertainty.** The following is an example from scenario 1 using Fisher Information.

![](./images/matrix_library.png){fig-align="center"}


## Assessing uncertainties across replicates {.scrollable}

:::{.panel-tabset}
# Single uncertainty
![](./images/walk_step_fisher_metrics_dotplot.png)

# Two combined
![](./images/walk_step_fisher_two_dim_metrics_mean_plot.png)

# All three!
![](./images/walk_step_fisher_three_dim_grid_plot.png){height="70%"}
:::


## Application to empirical

Simulation methods can be integrated with empirical studies to:

-	a priori help shape field sampling methods: e.g., number of core samples (across a region or local replication) required for a given research question.

- understand the sub-sampling and count resolution required to increase the likelihood of detecting a hypothesised signal in the data.

- accompany empirical study to test hypotheses about the underlying dynamics that may cause an observed pattern in the data.

- assess whether inferences made from the data are robust to uncertainty.


## What I haven't covered

"All models are wrong, some are useful" @box1979

- Assessing error rates

- Chronological uncertainty

- Extend underlying dynamics to ask specific questions (e.g., resilience)

## Acknowledgements

- George Perry (University of Auckland) and Janet Wilmshurs (University of Auckland, and Manaaki Whenua â€“ Landcare Research)

- Jack Williams and Tony Ives (University of Wisconsin Madison)

- Biological heritage Science Challenge (NZ) and the National Science Foundation (USA)
 

## References